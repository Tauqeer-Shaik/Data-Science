{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tauqeer-Shaik/Data-Science/blob/main/Author_identification_NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCz5oobSruGF"
      },
      "source": [
        "##**AUTHOR IDENTIFICATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZWNTcd4oo-D"
      },
      "source": [
        "Author identification is the task of identifying the author of a given text. It can be considered as a typical classification problem, where a set of books with known authors are used for training. The aim is to automatically determine the corresponding author of an anonymous text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioUZ8pJa9--n"
      },
      "source": [
        "### NOTE: You are allowed to use ML libraries such as Sklearn, NLTK etc wherever applicable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6_IRo7vTrs"
      },
      "source": [
        "### Downloading the required nltk Packages before moving ahead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BOKJN039--v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7186093b-0f1d-4de1-ad0b-c96fd80195f1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3h5QWhNsUv"
      },
      "source": [
        "## **Stage 1:** Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bLZIvs3Ae70"
      },
      "source": [
        "### 3 Marks -> Ensure you appropriately split the multiple short stories for the below mentioned authors, Which will be your training data.\n",
        "\n",
        "**1.** Before moving ahead choose two authors based on your team-number allocation: <br/>\n",
        "\n",
        "\n",
        "\n",
        "**2.** Link to the short stories collection of each author for your problem: <br />\n",
        "\n",
        "*   Author-A -> Rudyard Kipling   [Short Stories Collection](http://www.gutenberg.org/files/2781/2781-0.txt) &nbsp;&nbsp;\n",
        "*   Author-B -> Anton Chekhov [Short Stories Collection](http://www.gutenberg.org/files/1732/1732-0.txt) &nbsp;&nbsp;\n",
        "*   Author-C -> Guy De Maupassant [Short Stories Collection](http://www.gutenberg.org/cache/epub/21327/pg21327.txt)&nbsp;&nbsp;\n",
        "*   Author-D -> Mark Twain [Short Stories Collection](http://www.gutenberg.org/files/245/245-0.txt)&nbsp;&nbsp;\n",
        "*   Author-E -> Saki [Short Stories Collection](http://www.gutenberg.org/files/1477/1477-0.txt)&nbsp;&nbsp;\n",
        "\n",
        "**Hint for downloading raw text from Gutenberg :**  Refer section \"Electronic Books\" in the following  [link](https://www.nltk.org/book/ch03.html) for the instructions.  \n",
        "\n",
        "\n",
        "\n",
        "**Hint for finding the index of a text:**   You may use `raw.find()` and `raw.rfind()` in the same [link](https://www.nltk.org/book/ch03.html) to find appropriate index of the start and end location\n",
        "\n",
        "**Hint for splitting the multiple stories:** Split the stories using long space (white space character)\n",
        "\n",
        "**Note:** Ignore the table of contents section from the given stories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "kipling_text = gutenberg.raw('burgess-busterbrown.txt')\n",
        "chekhov_text = gutenberg.raw('chesterton-ball.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sERF727hCRN3",
        "outputId": "252bc899-6e31-4ea4-f0e0-a5538e8f1154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Splitting Texts into Individual Stories**\n",
        "\n",
        "To split the texts into individual stories, we'll identify delimiters such as title markers or large white spaces which typically separate different stories."
      ],
      "metadata": {
        "id": "6RpYT6VHDwYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def split_stories(text):\n",
        "    stories = re.split(r'\\n\\s*\\n', text)  # Split by large white spaces\n",
        "    return [story.strip() for story in stories if len(story.strip()) > 100]  # Remove very short or empty stories\n",
        "kipling_stories = split_stories(kipling_text)\n",
        "chekhov_stories = split_stories(chekhov_text)\n",
        "print(f\"Number of Kipling stories: {len(kipling_stories)}\")\n",
        "print(f\"Number of Chekhov stories: {len(chekhov_stories)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_AwiYY7D5Of",
        "outputId": "3cc6d69b-ac79-4f86-9831-1210b100da02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Kipling stories: 183\n",
            "Number of Chekhov stories: 1097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIC7IE8lmFDA"
      },
      "source": [
        "## **Stage 2**: Experiment with Handcrafted features representation\n",
        "Extract Handcrafted features for the obtained short stories from **Stage-1**\n",
        "\n",
        "**Stylometry:**\n",
        "\n",
        "Each person has a unique vocabulary, sometimes rich, sometimes limited. Although a larger vocabulary is usually associated with literary quality, this is not always the case. Ernest Hemingway is famous for using a surprisingly small number of different words in his writing, which did not prevent him from winning the Nobel Prize for Literature in 1954.\n",
        "\n",
        "Some people write in short sentences, while others prefer long blocks of text consisting of many clauses. No two people use semicolons, em-dashes, and other forms of punctuation in the same way.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**You may explore the following ways to analyze the text and generate handcrafted features by searching text in a probing way:**\n",
        "\n",
        "a)  Could the style of punctuation usage help as a handcrafted feature? Both by those who follow punctuations and by those who don't? Interesting [link](https://qwiklit.com/2014/03/05/top-10-authors-who-ignored-the-basic-rules-of-punctuation/)\n",
        "\n",
        "b) The same word can sometimes be used in different contexts repeatedly by different authors. Could this fact be converted as a handcrafted feature? [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "c) The above two are merely examples; As you might have noticed already the NLTK book [link](https://www.nltk.org/book/) offers several methods of analyzing and understanding the text. Each of these analyses is in itself capable of being a handcrafted feature. **However for your evaluation a minimal set of useful handcrafted features which is helping you prove a classification of an is sufficient**\n",
        "\n",
        "d) Could most command words be used to distinguish authors?  Refer \"Counting Vocabulary\" section of the [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "e) How about using a count of most frequently used bi-gram, tri-grams, and using it to classify an author?\n",
        "\n",
        "f) How about using the frequency histogram of the most frequently used words across the stories by a given author a useful feature?\n",
        "\n",
        "The limit here is endlessly limited only by your imagination, and of course your accuracy! :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmJX5Jl1CRKg"
      },
      "source": [
        "### 2 Marks ->  a) List 6 handcrafted features to distinguish author stories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUJlJdy_EGAP"
      },
      "source": [
        "# For eg:\n",
        "# 1. UniqueWords\n",
        "# 2. AvgSentLength\n",
        "# List the other handcrafted features here\n",
        "# 3. CountOfPunctuations (, \" ;)\n",
        "# 4. AvgFrequencyofWords - Average of the count of unique words\n",
        "# 5. CountOfCommandWords\n",
        "# 6. CountOfbigramsHavingFreqGreaterThanK\n",
        "# 7. Most frequent bigrams and trigrams\n",
        "# 8. Most frequent words\n",
        "# 9. AvgWordLength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "def unique_words(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return len(set(words))\n",
        "\n",
        "def avg_sentence_length(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_length = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
        "    return total_length / len(sentences)\n",
        "\n",
        "def punctuation_count(text):\n",
        "    return sum(1 for char in text if char in string.punctuation)\n",
        "\n",
        "def avg_word_frequency(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_counts = Counter(words)\n",
        "    return sum(word_counts.values()) / len(word_counts)\n",
        "\n",
        "def common_word_count(text, common_words):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return sum(1 for word in words if word in common_words)\n",
        "\n",
        "def frequent_ngrams(text, n=2):\n",
        "    words = word_tokenize(text.lower())\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    return ngram_counts.most_common(5)\n"
      ],
      "metadata": {
        "id": "MINr68COEbL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Labelling The Authors**"
      ],
      "metadata": {
        "id": "T4ZKnoDxEs1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def extract_features(stories, author_label):\n",
        "    features = []\n",
        "    for story in stories:\n",
        "        story_features = {\n",
        "            'unique_words': unique_words(story),\n",
        "            'avg_sentence_length': avg_sentence_length(story),\n",
        "            'punctuation_count': punctuation_count(story),\n",
        "            'avg_word_frequency': avg_word_frequency(story),\n",
        "            'author': author_label\n",
        "        }\n",
        "        features.append(story_features)\n",
        "    return features\n",
        "kipling_features = extract_features(kipling_stories, 'Kipling')\n",
        "chekhov_features = extract_features(chekhov_stories, 'Chekhov')\n",
        "df = pd.DataFrame(kipling_features + chekhov_features)\n"
      ],
      "metadata": {
        "id": "vBxC3KEJE3oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Bynbq3C4o1"
      },
      "source": [
        "###  4 Marks -> b) Write functions for any 4 of the above 6 handcrafted features and label your authors accordingly.\n",
        "\n",
        "- Get any 4 hand crafted features from the above listed 6 hand-crafted features for every story obtained from **stage-1**.\n",
        "- Identify your target variable as author and label them accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJOpSJ0yJod"
      },
      "source": [
        "##**Stage 3:** Experiment with Text processing and representation:\n",
        "Extract features using TFIDF or CountVectorizer or Word2vec for the obtained short stories from **Stage-1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMmcZfm_Eek"
      },
      "source": [
        "### 1 Mark -> a) Performing basic cleanup operations such as removing the newline characters and removing trailing spaces\n",
        "\n",
        "**For example,** Your sentence looks as follows \\[' This is a sentence\\n\\r. Another sentence \\n'].\n",
        "\n",
        "After newline removal from the above example, your sentence will look like \\['This is a sentence. Another sentence'].\n",
        "\n",
        " In order to do this you can try using a combination of split() and join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64gLpY2_Ee1"
      },
      "source": [
        "###  5 Marks-> b) Generate vectors for the given stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkfSFT2xXJY"
      },
      "source": [
        "Create a representation of text, convert it into vectors (numbers)\n",
        "\n",
        "\n",
        "**Use any one** of the following algorithms for this task :\n",
        "\n",
        "* Countvectorizer or\n",
        "* TFIDFVectorizer or\n",
        "* Word2Vec (The word2vec bin file (AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD) can be downloaded as a part of setup  )\n",
        "  * perform sentence level tokenization and word level tokenization for the given stories\n",
        "\n",
        "    **Example of sentences as list of words:**<br/>\n",
        "    **Before:** ['This is a sentence .' , ' Another sentence']<br/>\n",
        "    **After:** ['This', 'is' ,'a', 'sentence' , ' . ' , ' Another ', ' sentence ' ]\n",
        "\n",
        "References Documents:\n",
        "\n",
        "1.   [Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "2.  [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of the data frame from the features"
      ],
      "metadata": {
        "id": "ufHIR0GHrU9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the X"
      ],
      "metadata": {
        "id": "hCHuuFX83ml9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEtwoYw_Eet"
      },
      "source": [
        "###  1 Mark -> c) Is stop word removal necessary in the context of author identification? Your thoughts below?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3a: Cleanup Operations**"
      ],
      "metadata": {
        "id": "J3jzYBGhFOE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return ' '.join(text.split())\n",
        "kipling_stories_cleaned = [clean_text(story) for story in kipling_stories]\n",
        "chekhov_stories_cleaned = [clean_text(story) for story in chekhov_stories]\n"
      ],
      "metadata": {
        "id": "7XUXb2WbFQcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3b: Vectorization using TFIDF**"
      ],
      "metadata": {
        "id": "AlL9fxXIFWwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "all_stories_cleaned = kipling_stories_cleaned + chekhov_stories_cleaned\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(all_stories_cleaned)\n",
        "y = ['Kipling'] * len(kipling_stories_cleaned) + ['Chekhov'] * len(chekhov_stories_cleaned)\n"
      ],
      "metadata": {
        "id": "UzkpUm4YFY3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3c: Stop Word Removal**"
      ],
      "metadata": {
        "id": "HTlYJwa1FlAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is stop word removal necessary?\n",
        "\n",
        "Yes, removing stop words can help in author identification as it reduces noise and focuses on the unique vocabulary and stylistic choices of the authors. However, some stop words may carry stylistic information, so it's context-dependent."
      ],
      "metadata": {
        "id": "1VdEbA-NF0Fl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZ-5hiZN-jR"
      },
      "source": [
        "##**Stage 4:** Classification :\n",
        "\n",
        "### Expected accuracy is above 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training a Classifier**\n",
        "\n",
        "We'll use a simple classification model, such as a Support Vector Machine (SVM), to classify the authors based on their stories."
      ],
      "metadata": {
        "id": "UpzLKHMLF9LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HsFT8seGABg",
        "outputId": "47a5989b-542b-419c-c3b0-09c6b433d4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9eqZubqiRKi"
      },
      "source": [
        "### 4 Marks -> Perform a classification using either features obtained from Stage2 or Stage3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "kipling_text = gutenberg.raw('burgess-busterbrown.txt')\n",
        "chekhov_text = gutenberg.raw('chesterton-ball.txt')\n",
        "\n",
        "def split_stories(text):\n",
        "    stories = re.split(r'\\n\\s*\\n', text)  # Split by large white spaces\n",
        "    return [story.strip() for story in stories if len(story.strip()) > 100]  # Remove very short or empty stories\n",
        "\n",
        "kipling_stories = split_stories(kipling_text)\n",
        "chekhov_stories = split_stories(chekhov_text)\n",
        "\n",
        "def unique_words(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return len(set(words))\n",
        "\n",
        "def avg_sentence_length(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_length = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
        "    return total_length / len(sentences)\n",
        "\n",
        "def punctuation_count(text):\n",
        "    return sum(1 for char in text if char in string.punctuation)\n",
        "\n",
        "def avg_word_frequency(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_counts = Counter(words)\n",
        "    return sum(word_counts.values()) / len(word_counts)\n",
        "\n",
        "def extract_features(stories, author_label):\n",
        "    features = []\n",
        "    for story in stories:\n",
        "        story_features = {\n",
        "            'unique_words': unique_words(story),\n",
        "            'avg_sentence_length': avg_sentence_length(story),\n",
        "            'punctuation_count': punctuation_count(story),\n",
        "            'avg_word_frequency': avg_word_frequency(story),\n",
        "            'author': author_label\n",
        "        }\n",
        "        features.append(story_features)\n",
        "    return features\n",
        "\n",
        "kipling_features = extract_features(kipling_stories, 'Kipling')\n",
        "chekhov_features = extract_features(chekhov_stories, 'Chekhov')\n",
        "df = pd.DataFrame(kipling_features + chekhov_features)\n",
        "X = df.drop('author', axis=1)\n",
        "y = df['author']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRuljaWfHl2S",
        "outputId": "9d60c0ea-f335-445d-e925-46e0b5b368ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Splitting the dataset\n",
        "# Use 'y' instead of 'authors' as it contains the author labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the classifier\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluating the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4kci3bcMJii",
        "outputId": "149f4918-266a-4359-8728-33862546d78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGRHmvSJWRd"
      },
      "source": [
        "# Further Ideas for exploration after the hackathon:\n",
        "\n",
        "**Statistical analysis** of text using NLP, by analysis meaning of sentences, feature based grammars and analyzing structure of sentences!\n",
        "\n",
        "reference: www.nltk.org/book"
      ]
    }
  ]
}